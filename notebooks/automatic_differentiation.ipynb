{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Automatic differentiation (AD)__ is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program. \n",
    "\n",
    "Automatic differentiation is __NOT__:\n",
    "- Symbolic differentiation, nor\n",
    "- Numerical differentiation (the method of finite differences).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/AutomaticDifferentiationNutshell.png/1920px-AutomaticDifferentiationNutshell.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as anp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd.scipy.signal import convolve\n",
    "\n",
    "def softmax(x):    return anp.exp(x - logsumexp(x))\n",
    "def softplus(x):   return anp.logaddexp(0., x)\n",
    "def sigmoid(x):    return anp.reciprocal(anp.exp(softplus(-x)))\n",
    "def flip(x):       return x[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Элементарные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_transform(x, mat, bias):\n",
    "    return np.dot(mat,x) + bias\n",
    "\n",
    "def subassign(x, y, idx_dst, idx_src):\n",
    "    assert(len(idx_dst) == len(idx_src))\n",
    "    z = x.copy()\n",
    "    z[idx_dst] = y[idx_src]\n",
    "    return z\n",
    "\n",
    "primitive_functions_dict = dict()\n",
    "\n",
    "primitive_functions_dict['negative'] = lambda x: -1.*x\n",
    "primitive_functions_dict['exponential'] = lambda x,a: np.power(a,x)\n",
    "primitive_functions_dict['powexp'] = lambda x,y: np.power(x,y)\n",
    "primitive_functions_dict['exp'] = lambda x: np.exp(x)\n",
    "primitive_functions_dict['log'] = lambda x: np.log(x)\n",
    "primitive_functions_dict['sin'] = lambda x: np.sin(x)\n",
    "primitive_functions_dict['cos'] = lambda x: np.cos(x)\n",
    "primitive_functions_dict['power'] = lambda x,p: np.power(x,p)\n",
    "primitive_functions_dict['sub'] = lambda x,idx: x[idx]\n",
    "primitive_functions_dict['subassign'] = lambda x,y,idx_dst,idx_src: subassign(x, y, idx_dst, idx_src)\n",
    "primitive_functions_dict['flip'] = lambda x: np.flip(x)\n",
    "primitive_functions_dict['diff'] = lambda x: np.diff(x)\n",
    "primitive_functions_dict['cumsum'] = lambda x: np.cumsum(x)\n",
    "primitive_functions_dict['concat'] = lambda *vectors: np.concatenate(vectors)\n",
    "primitive_functions_dict['conv_full'] = lambda x, kernel: convolve(x, kernel, mode='full')\n",
    "primitive_functions_dict['conv_valid'] = lambda x, kernel: convolve(x, kernel, mode='valid')\n",
    "primitive_functions_dict['reciprocal'] = lambda x: 1./x\n",
    "primitive_functions_dict['logsumexp'] = lambda x: logsumexp(x)\n",
    "primitive_functions_dict['softmax'] = lambda x: softmax(x)\n",
    "primitive_functions_dict['softplus'] = lambda x: softplus(x)\n",
    "primitive_functions_dict['sigmoid'] = lambda x: sigmoid(x)\n",
    "primitive_functions_dict['inner'] = lambda x,y: np.inner(x,y)\n",
    "primitive_functions_dict['add'] = lambda x,y: x+y\n",
    "primitive_functions_dict['subtract'] = lambda x,y: x-y\n",
    "primitive_functions_dict['multiply'] = lambda x,y: x*y\n",
    "primitive_functions_dict['divide'] = lambda x,y: x/y\n",
    "primitive_functions_dict['affine'] = lambda x, *params: affine_transform(x, *params)\n",
    "primitive_functions_dict['scalar_mult_add'] = lambda x, *params: affine_transform(x, *params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VJP для элементарных функций. \n",
    "\n",
    "\n",
    "Функция vjp(vec, ans, \\*inputs, *params) принимает на вход\n",
    "- vec - вектор, $v$\n",
    "- ans - значение функции для заданных входных аргументов, $f(x,y,z,...)$\n",
    "- *inputs - входные аргументы, $x,y,z,...$\n",
    "- *params - параметры функции\n",
    "\n",
    "и возвращает список\n",
    "\n",
    "$$v^T \\frac{\\partial f}{\\partial x}, v^T \\frac{\\partial f}{\\partial y}, v^T \\frac{\\partial f}{\\partial z}, ...$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vjp_affine_transform(vec, mat, bias):\n",
    "    return np.dot(vec, mat)\n",
    " \n",
    "vjps_dict = dict()\n",
    "\n",
    "vjps_dict['negative'] = lambda vec, ans, x: [-1.*vec]\n",
    "vjps_dict['exponential'] = lambda vec, ans, x, a: [vec*np.log(a)*ans]\n",
    "vjps_dict['exp'] = lambda vec, ans, x: [vec*ans]\n",
    "vjps_dict['log'] = lambda vec, ans, x: [vec/x]\n",
    "vjps_dict['sin'] = lambda vec, ans, x: [vec*np.cos(x)]\n",
    "vjps_dict['cos'] = lambda vec, ans, x: [-1.*vec*np.sin(x)]\n",
    "vjps_dict['power'] = lambda vec, ans, x, p: [vec*p*np.power(x,p-1)]\n",
    "vjps_dict['inner'] = lambda vec, ans, x,y: [vec*y, vec*x]\n",
    "vjps_dict['add'] = lambda vec, ans, x,y: [vec, vec]\n",
    "vjps_dict['subtract'] = lambda vec, ans, x,y: [vec, -1.*vec]\n",
    "vjps_dict['affine'] = lambda vec, ans, x, *params: [vjp_affine_transform(vec, *params)]\n",
    "vjps_dict['scalar_mult_add'] = lambda vec, ans, x, *params: [vjp_affine_transform(vec, *params)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямой проход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, graph, nodes_sorted):\n",
    "           \n",
    "    for node in nodes_sorted:\n",
    "        func = graph.node[node]['function']\n",
    "        parents = graph.node[node]['parents']\n",
    "        args = []\n",
    "        for p in parents:\n",
    "            args.append(graph.node[p]['value'])\n",
    "        if 'params' in graph.node[node]:\n",
    "            args = args + [*graph.node[node]['params']]\n",
    "\n",
    "        if len(args) > 0:\n",
    "            graph.node[node]['value'] = primitive_functions_dict[func](*args)\n",
    "        else:\n",
    "            graph.node[node]['value'] = x\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратный проход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jacs(prev_jac, jac):\n",
    "    if prev_jac is None:\n",
    "        return jac\n",
    "    else:\n",
    "        return prev_jac + jac\n",
    "\n",
    "def backward_pass(graph, nodes_sorted):\n",
    "    \n",
    "    nodes_sorted_backward = nodes_sorted[::-1]\n",
    "    end_node = nodes_sorted_backward[0]\n",
    "    jacs = {end_node: 1.0}\n",
    "    \n",
    "    for node in nodes_sorted_backward:\n",
    "        func = graph.node[node]['function']\n",
    "        value = graph.node[node]['value']\n",
    "        parents = graph.node[node]['parents']\n",
    "        \n",
    "        jac = jacs.pop(node)\n",
    "        \n",
    "        args = []\n",
    "        for p in parents:\n",
    "            args.append(graph.node[p]['value'])\n",
    "        if 'params' in graph.node[node]:\n",
    "            args = args + [*graph.node[node]['params']]\n",
    "            \n",
    "        if len(args) > 0:\n",
    "            parent_jacs = vjps_dict[func](jac, value, *args) \n",
    "        \n",
    "        for i in range(len(parents)): \n",
    "            p = parents[i]\n",
    "            jacs.update({p: add_jacs(jacs.get(p), parent_jacs[i])})\n",
    "            \n",
    "    return jac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_function_and_gradient(computational_graph):\n",
    "    \n",
    "    nodes_sorted = list(nx.topological_sort(computational_graph))\n",
    "    end_node = nodes_sorted[-1]\n",
    "    \n",
    "    def function(x):\n",
    "        graph = computational_graph.copy()\n",
    "        forward_pass(x, graph, nodes_sorted)\n",
    "        f_x = graph.node[end_node]['value']\n",
    "        return f_x\n",
    "    \n",
    "    def gradient(x):\n",
    "        graph = computational_graph.copy()\n",
    "        forward_pass(x, graph, nodes_sorted)\n",
    "        g_x = backward_pass(graph, nodes_sorted)\n",
    "        return g_x\n",
    "    \n",
    "    return function, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность входа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение функции, $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "A = np.random.randn(dim,dim)\n",
    "b = np.random.randn(dim,)\n",
    "\n",
    "def function_0(x):\n",
    "    z = anp.dot(A,x) + b\n",
    "    u = anp.exp(z) + 5*anp.sin(x) + 3\n",
    "    return anp.dot(u.T,u)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание графа вычислений для $f$. Каждая вершина графа имеет следующие атрибуты:\n",
    "- 'function' - элементарная функция\n",
    "- 'value' - результат вычисления значения элементарной функции\n",
    "- 'parents' - список вершин-предков, длина которого равна количеству аргументов элементарной функции\n",
    "- 'params' - параметры элементарной функции\n",
    "\n",
    "Атрибут 'value' инициализируется как None; его значение вычисляется во время прямого прохода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_node(0, function=None, value=None, parents=[])\n",
    "G.add_node(1, function='affine', value=None, parents=[0], params=[A,b])\n",
    "G.add_node(2, function='exp', value=None, parents=[1])\n",
    "G.add_node(3, function='sin', value=None, parents=[0])\n",
    "G.add_node(4, function='scalar_mult_add', value=None, parents=[3], params=[5,3])\n",
    "G.add_node(5, function='add', value=None, parents=[2,4])\n",
    "G.add_node(6, function='inner', value=None, parents=[5,5])\n",
    "G.add_node(7, function='power', value=None, parents=[6], params=[0.5])\n",
    "\n",
    "G.add_edges_from([(0,1),(1,2),(0,3),(3,4),(2,5),(4,5),(5,6),(6,7)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение функций, которые вычисляют $f(x)$ и $\\nabla f(x)$ для любого $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function, gradient = make_function_and_gradient(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение функций, которые вычисляют $f(x)$ и $\\nabla f(x)$ для любого $x$; с помощью библиотеки [autograd](https://github.com/HIPS/autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "\n",
    "gradient_0 = grad(function_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входной вектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(dim,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка корректности вычисления $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407.0560137190764\n",
      "407.0560137190764\n",
      "Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "f_true = function_0(x)\n",
    "f = function(x)\n",
    "\n",
    "print(f_true)\n",
    "print(f)\n",
    "print('Difference:', np.abs(f_true - f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка корректности вычисления $\\nabla f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-230.57270198  658.79279437 -967.44412286 -171.04452494]\n",
      "[-230.57270198  658.79279437 -967.44412286 -171.04452494]\n",
      "Relative difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import norm\n",
    "\n",
    "g_true = gradient_0(x)\n",
    "g = gradient(x)\n",
    "\n",
    "print(g_true)\n",
    "print(g)\n",
    "print('Relative difference:', norm(g_true - g)/norm(g_true + g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка корректности вычисления $\\nabla f(x)$ на основе численного приближения\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + e_i \\epsilon) - f(x)}{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.719044814246033e-05\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "difference = check_grad(function, gradient, x)\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
