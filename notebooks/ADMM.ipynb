{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADMM\n",
    "\n",
    "http://statsmaths.github.io/stat612/lectures/lec23/lecture23.pdf\n",
    "\n",
    "http://www.stronglyconvex.com/blog/admm-revisited.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO\n",
    "\n",
    "Solving the Lasso via ADMM:\n",
    "\n",
    "https://statweb.stanford.edu/~candes/math301/Lectures/Consensus.pdf\n",
    "\n",
    "$$ \\min_{\\mathbf{x}} \\frac{1}{2} {\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert}^2_2 + \\lambda {\\lVert \\mathbf{x} \\rVert}_1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy.sparse.linalg import cg, lsqr\n",
    "from scipy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_l1(x, tau):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - tau, 0)\n",
    "\n",
    "def lasso_admm(A, b, lmbda, rho=0.1, max_iter=10000):\n",
    "    \"\"\"Solves\n",
    "    0.5*||A*x - y||^2_2 + lmbda*||x||_1\n",
    "    using ADMM\n",
    "    \"\"\"\n",
    "    x0 = np.random.randn(A.shape[1],)\n",
    "    n = len(x0)\n",
    "    x = x0.copy()\n",
    "    z = x0.copy()\n",
    "    u = x0.copy()\n",
    "    \n",
    "    is_converged = False\n",
    "    for iteration in range(max_iter):\n",
    "        # update x\n",
    "        mv = lambda x: A.rmatvec(A.matvec(x)) + rho*x\n",
    "        L = LinearOperator((n,n), matvec=mv)\n",
    "        x = cg(L, A.rmatvec(b) + rho*(z - u))[0]\n",
    "        # update z\n",
    "        z = prox_l1(x + u, lmbda/rho)\n",
    "        # update u\n",
    "        r = x - z\n",
    "        u = u + rho*(r)   \n",
    "        # check convergence\n",
    "        if norm(r)/np.max([norm(x), norm(z)]) < 1e-14:\n",
    "            is_converged = True\n",
    "            break\n",
    "    if not is_converged:\n",
    "        print('Warning! Convergence criterion is not satisfied!')\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.         -0.         -0.67691334  0.          0.        ]\n",
      "[ 0.          0.         -0.67691334  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "from pyunlocbox import functions\n",
    "\n",
    "norm_l1 = functions.norm_l1()\n",
    "\n",
    "x0 = np.random.randn(5,)\n",
    "tau = 0.7\n",
    "print(prox_l1(x0, tau))\n",
    "print(norm_l1.prox(x0, tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def lasso_sklearn(A, b, lmbda):\n",
    "    \"\"\"Solves\n",
    "    0.5 / n_samples * ||A*x - y||^2_2 + alpha * ||x||_1,\n",
    "    where lmbda = alpha*n_samples\n",
    "    \"\"\"\n",
    "    alpha = lmbda/A.shape[0]\n",
    "    rgr_lasso = Lasso(alpha=alpha, fit_intercept=False)\n",
    "    rgr_lasso.fit(A, b)\n",
    "    return rgr_lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "A_ = np.random.randn(3,5)\n",
    "\n",
    "def mv(v):\n",
    "    return A_.dot(v)\n",
    "def rmv(v):\n",
    "    return A_.T.dot(v)\n",
    "\n",
    "A = LinearOperator(A_.shape, matvec=mv, rmatvec=rmv)\n",
    "b = np.random.randn(A_.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.60737356e-01 -4.07074992e-15 -5.95278300e-01  1.29171077e-15\n",
      "  1.18008980e+00]\n"
     ]
    }
   ],
   "source": [
    "lmbda = 0.1\n",
    "x_opt_1 = lasso_admm(A, b, lmbda)\n",
    "print(x_opt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.76062233 -0.         -0.59557565  0.          1.18016384]\n"
     ]
    }
   ],
   "source": [
    "lmbda = 0.1\n",
    "x_opt_2 = lasso_sklearn(A_, b, lmbda)\n",
    "print(x_opt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPCA\n",
    "\n",
    "Robust Principal Component Analysis\n",
    "\n",
    "\\begin{gather}\n",
    "\\min_{\\mathbf{L}, \\mathbf{S}} \\lVert \\mathbf{L} \\rVert_* + \\lambda \\lVert \\mathbf{S} \\rVert_1  \\\\\n",
    "\\text{s.t. } \\mathbf{X} = \\mathbf{L} + \\mathbf{S}\n",
    "\\end{gather}\n",
    "\n",
    "https://statweb.stanford.edu/~candes/math301/Lectures/rpca.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_nuclear(X, tau):\n",
    "    [U, S, V] = np.linalg.svd(X, full_matrices=False)\n",
    "    S = np.maximum(S - tau, 0)\n",
    "    S = np.diag(S)\n",
    "    return U.dot(S).dot(V)\n",
    "\n",
    "def RPCA_admm(X, lmbda, rho=0.1, max_iter=10000):\n",
    "    \"\"\"Solves RPCA via ADMM\n",
    "    \"\"\"\n",
    "    M,N = X.shape\n",
    "    normX = norm(X, 'fro')\n",
    "    \n",
    "    # initial solution\n",
    "    L = np.zeros((M, N))\n",
    "    S = np.zeros((M, N))\n",
    "    U = np.zeros((M, N))\n",
    "    \n",
    "    is_converged = False\n",
    "    for iteration in range(max_iter):          \n",
    "        # update L and S\n",
    "        L = prox_nuclear(X - S + (1/rho)*U, 1/rho)\n",
    "        S = prox_l1(X - L + (1/rho)*U, lmbda/rho)\n",
    "        # update U\n",
    "        Z = X - L - S\n",
    "        U = U + rho*Z\n",
    "        # check convergence\n",
    "        err = norm(Z, 'fro') / normX      \n",
    "        if err < 1e-12:\n",
    "            is_converged = True\n",
    "            break\n",
    "    if not is_converged:\n",
    "        print('Warning! Convergence criterion is not satisfied!')\n",
    "    return L, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21367621 -0.05831636  0.03830712]\n",
      " [-0.24143197 -0.06589144  0.04328308]\n",
      " [-0.40432885 -0.11034915  0.07248666]]\n",
      "[[-0.21367621 -0.05831636  0.03830712]\n",
      " [-0.24143197 -0.06589144  0.04328308]\n",
      " [-0.40432885 -0.11034915  0.07248666]]\n"
     ]
    }
   ],
   "source": [
    "norm_nuc = functions.norm_nuclear()\n",
    "\n",
    "X0 = np.random.randn(3,3)\n",
    "tau = 1.3\n",
    "print(prox_nuclear(X0, tau))\n",
    "print(norm_nuc.prox(X0, tau)) # works only for square matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4 \n",
    "n = 3\n",
    "\n",
    "np.random.seed(123)\n",
    "base = 100 + np.cumsum(np.random.randn(m,n),axis=0)\n",
    "scales = np.abs(np.random.randn(m,n))\n",
    "\n",
    "L = np.dot(base, scales.T)\n",
    "S = np.round(0.25 * np.random.randn(m,m))\n",
    "X = L + S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sporco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[253.97359246 485.05408746 212.26467636 359.88719381]\n",
      " [252.67146505 482.56720599 211.17639135 358.04204537]\n",
      " [251.35706571 480.05688684 210.07784977 356.17950728]\n",
      " [249.58862727 476.67941644 208.59983386 353.67358396]]\n",
      "[[2.59751091 0.         0.         0.        ]\n",
      " [2.01670518 4.16760718 0.57015378 0.        ]\n",
      " [0.         7.44607564 0.         0.33214328]\n",
      " [0.         8.7423926  0.27574982 0.7993027 ]]\n"
     ]
    }
   ],
   "source": [
    "from sporco.admm import rpca\n",
    "\n",
    "options = rpca.RobustPCA.Options({'Verbose': False, 'gEvalY': False,'MaxMainIter': 10000, \n",
    "                                  'RelStopTol': 1e-12, 'AutoRho': {'Enabled': True}})\n",
    "\n",
    "lmbda = 0.4\n",
    "rpca = rpca.RobustPCA(X, lmbda, options)\n",
    "L_opt_1, S_opt_1 = rpca.solve()\n",
    "print(L_opt_1)\n",
    "print(S_opt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[253.97359246 485.05408746 212.26467636 359.88719381]\n",
      " [252.67146505 482.56720599 211.17639135 358.04204537]\n",
      " [251.35706571 480.05688685 210.07784978 356.17950728]\n",
      " [249.58862728 476.67941644 208.59983386 353.67358396]]\n",
      "[[ 2.59751092  0.          0.          0.        ]\n",
      " [ 2.01670518  4.16760718  0.57015378 -0.        ]\n",
      " [ 0.          7.44607563 -0.          0.33214328]\n",
      " [-0.          8.7423926   0.27574982  0.7993027 ]]\n"
     ]
    }
   ],
   "source": [
    "lmbda = 0.4\n",
    "L_opt_2, S_opt_2 = RPCA_admm(X, lmbda)\n",
    "print(L_opt_2)\n",
    "print(S_opt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear programming\n",
    "\n",
    "Solving standard form LP via ADMM\n",
    "\n",
    "\\begin{gather}\n",
    "\\min_{\\mathbf{x}}  \\mathbf{c}^T \\mathbf{x}  \\\\\n",
    "\\text{s.t. } \\mathbf{G}\\mathbf{x} = \\mathbf{h}, \\mathbf{x} \\geq \\mathbf{0}  \n",
    "\\end{gather}\n",
    "\n",
    "http://cermics.enpc.fr/~parmenta/frejus/Summerproject2018-4.pdf\n",
    "\n",
    "https://github.com/nmchaves/admm-for-lp/blob/master/report/fall2016/report.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 5\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "c = np.random.randn(dim,)\n",
    "\n",
    "G = np.random.randn(2,dim)\n",
    "h = np.random.randn(2,)\n",
    "\n",
    "A = -1*np.eye(dim)\n",
    "b = np.zeros((dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.76992000e-17]\n",
      " [ 5.09006281e-01]\n",
      " [ 9.90548429e-01]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# objective\n",
    "c_ = matrix(c)\n",
    "# inequalities\n",
    "A_ = matrix(A)\n",
    "b_ = matrix(b)\n",
    "# equalities\n",
    "G_ = matrix(G)\n",
    "h_ = matrix(h)\n",
    "\n",
    "sol=solvers.lp(c_, A_, b_, G_, h_, solver='glpk')\n",
    "x_opt_1 = np.array(sol['x'])\n",
    "\n",
    "print(x_opt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_admm(c, G, h, rho=0.1, max_iter=10000):\n",
    "    \"\"\"Solves LP via ADMM\n",
    "    \"\"\"\n",
    "    n,dim = G.shape\n",
    "    x1 = lsqr(G, h)[0]\n",
    "    x2 = x1.copy()\n",
    "    u = np.zeros((n,))\n",
    "    s = np.zeros((dim,))\n",
    "    \n",
    "    mv = lambda x: G.rmatvec(G.matvec(x)) + x\n",
    "    GTGeye = LinearOperator((dim, dim), matvec=mv)\n",
    "    GTh = G.rmatvec(h)\n",
    "    \n",
    "    is_converged = False\n",
    "    for iteration in range(max_iter):          \n",
    "        # update x1\n",
    "        x1 = cg(GTGeye, x2 + GTh - 1./rho*(c + G.rmatvec(u) + s))[0]\n",
    "        # update x2\n",
    "        x2 = np.maximum(x1 + 1./rho*s, 0)\n",
    "        # update u and s\n",
    "        u = u + rho*(G.matvec(x1) - h)\n",
    "        s = s + rho*(x1 - x2)\n",
    "        # check convergence\n",
    "        err = norm(x1 - x2)/np.max([norm(x1), norm(x2)])      \n",
    "        if err < 1e-12:\n",
    "            is_converged = True\n",
    "            break\n",
    "    if not is_converged:\n",
    "        print('Warning! Convergence criterion is not satisfied!')  \n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.87941457e-13  5.09006281e-01  9.90548429e-01  4.54997127e-14\n",
      " -5.04729366e-14]\n"
     ]
    }
   ],
   "source": [
    "def mv(v):\n",
    "    return G.dot(v)\n",
    "def rmv(v):\n",
    "    return G.T.dot(v)\n",
    "\n",
    "G_op = LinearOperator(G.shape, matvec=mv, rmatvec=rmv)\n",
    "\n",
    "x_opt_2 = lp_admm(c, G_op, h)\n",
    "print(x_opt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-constrained optimization\n",
    "\n",
    "\\begin{gather}\n",
    "\\min_{\\mathbf{x}}  f(\\mathbf{x})  \\\\\n",
    "\\text{s.t. } \\mathbf{x} \\in C\n",
    "\\end{gather}\n",
    "\n",
    "\\begin{gather}\n",
    "\\min_{\\mathbf{x}, \\mathbf{z}}  f(\\mathbf{x}) + \\mathbb{i}_C( \\mathbf{z} )  \\\\\n",
    "\\text{s.t. } \\mathbf{x} - \\mathbf{z} =  \\mathbf{0}\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\frac{1}{2} \\| \\mathbf{x} - \\mathbf{x}_0 \\|^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "from utils.simplex_projection import euclidean_proj_simplex\n",
    "\n",
    "def euclidean_proj_l1_ball(v, s=1):\n",
    "    u = np.abs(v)\n",
    "    if u.sum() <= s:\n",
    "        return v\n",
    "    w = euclidean_proj_simplex(u, s=s)\n",
    "    w *= np.sign(v)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def fmin_admm(f, grad, proj, x0, rho=0.2, max_iter=10000):\n",
    "    \n",
    "    x = x0.copy()\n",
    "    z = x0.copy()\n",
    "    u = x0.copy()\n",
    "    \n",
    "    is_converged = False\n",
    "    for iteration in range(max_iter):          \n",
    "        # update x\n",
    "        func = lambda v: f(v) + 0.5*rho*norm(v - z + u)**2\n",
    "        gradient = lambda v: grad(v) + rho*(v - z + u)\n",
    "        x = minimize(func, x, method='BFGS', jac=gradient, options={'maxiter': 100, 'disp': False}, tol=1e-12).x\n",
    "        # update z\n",
    "        z = proj(x + u)\n",
    "        # update u\n",
    "        u = u + x - z\n",
    "        # check convergence\n",
    "        err = norm(x - z)/np.max([norm(x), norm(z)])\n",
    "        if err < 1e-12:\n",
    "            is_converged = True\n",
    "            break\n",
    "    if not is_converged:\n",
    "        print('Warning! Convergence criterion is not satisfied!')  \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.  -0.1 -0. ]\n"
     ]
    }
   ],
   "source": [
    "dim = 4\n",
    "\n",
    "np.random.seed(10)\n",
    "x0 = np.random.randn(dim,)\n",
    "\n",
    "x_opt_1 = euclidean_proj_l1_ball(x0, 0.1)\n",
    "print(x_opt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.18922531e-14  6.57678355e-15 -1.00000000e-01 -7.40251142e-17]\n"
     ]
    }
   ],
   "source": [
    "projection = lambda x: euclidean_proj_l1_ball(x, 0.1)\n",
    "\n",
    "x_opt_2 = fmin_admm(lambda x: 0.5*norm(x - x0)**2, lambda x: x - x0, projection, x0)\n",
    "print(x_opt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus optimization\n",
    "\n",
    "Suppose we have a convex optimization problem with $N$ terms in the objective\n",
    "\n",
    "\\begin{array}{ll} \\mbox{minimize} & \\sum_{i=1}^N f_i(x)\\\\\n",
    "\\end{array}\n",
    "\n",
    "For example, we might be fitting a model to data and $f_i$ is the loss function for the $i$th block of training data.\n",
    "\n",
    "We can convert this problem into consensus form\n",
    "\n",
    "\\begin{array}{ll} \\mbox{minimize} & \\sum_{i=1}^N f_i(x_i)\\\\\n",
    "\\mbox{subject to} & x_i = z\n",
    "\\end{array}\n",
    "\n",
    "We interpret the $x_i$ as local variables, since they are particular to a given $f_i$. The variable $z$, by contrast, is global. The constraints $x_i = z$ enforce consistency, or consensus.\n",
    "\n",
    "We can solve a problem in consensus form using the Alternating Direction Method of Multipliers (ADMM). Each iteration of ADMM reduces to the following updates:\n",
    "\n",
    "\\begin{array}{lll}\n",
    "% xbar, u parameters in prox.\n",
    "% called proximal operator.\n",
    "x^{k+1}_i & := & \\mathop{\\rm argmin}_{x_i}\\left(f_i(x_i) + \\frac{\\rho}{2}\\left\\|x_i - \\overline{x}^k + u^k_i \\right\\|^2_2 \\right) \\\\\n",
    "% u running sum of errors.\n",
    "u^{k+1}_i & := & u^{k}_i + x^{k+1}_i - \\overline{x}^{k+1}\n",
    "\\end{array}\n",
    "\n",
    "where $\\overline{x}^k =  \\frac{1}{N}\\sum_{i=1}^N x^k_i$.\n",
    "\n",
    "http://proceedings.mlr.press/v32/zhange14.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "def consensus_admm(f_list, g_list, x0, rho=0.1, max_iter=1000):\n",
    "\n",
    "    N = len(f_list)\n",
    "\n",
    "    def run_worker(f, grad, pipe):\n",
    "        xbar = x0.copy()\n",
    "        u = x0.copy()\n",
    "        x = x0.copy()\n",
    "        while True:\n",
    "            func = lambda v: f(v) + 0.5*rho*norm(v - xbar + u)**2\n",
    "            gradient = lambda v: grad(v) + rho*(v - xbar + u)\n",
    "            x = minimize(func, x, method='BFGS', jac=gradient, options={'maxiter': 100, 'disp': False}, tol=1e-12).x\n",
    "            pipe.send(x)\n",
    "            xbar = pipe.recv()\n",
    "            u = u + x - xbar\n",
    "\n",
    "    # Setup the workers.\n",
    "    pipes = []\n",
    "    procs = []\n",
    "    for i in range(N):\n",
    "        local, remote = Pipe()\n",
    "        pipes += [local]\n",
    "        procs += [Process(target=run_worker, args=(f_list[i], g_list[i], remote))]\n",
    "        procs[-1].start()\n",
    "\n",
    "    is_converged = False\n",
    "    for iteration in range(max_iter):\n",
    "        # Gather and average xi\n",
    "        x_list = [pipe.recv() for pipe in pipes]\n",
    "        xbar = sum(x_list)/N\n",
    "        # Scatter xbar\n",
    "        for pipe in pipes:\n",
    "            pipe.send(xbar)\n",
    "        # check convergence\n",
    "        if np.all([np.allclose(xbar, xi, rtol=1e-8) for xi in x_list]):\n",
    "            is_converged = True\n",
    "            break\n",
    "            \n",
    "    if not is_converged:\n",
    "        print('Warning! Convergence criterion is not satisfied!') \n",
    "\n",
    "    [p.terminate() for p in procs]\n",
    "\n",
    "    return xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "A = np.random.randn(5,4)\n",
    "b = np.random.randn(5,)\n",
    "\n",
    "x0 = np.random.randn(4,)\n",
    "\n",
    "f1 = lambda x: 0.123*np.sum(x**2)/2\n",
    "f2 = lambda x: np.sum((np.dot(A,x) - b)**2)/2\n",
    "\n",
    "g1 = lambda x: 0.123*x\n",
    "g2 = lambda x: A.T.dot(np.dot(A,x) - b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6074129  -0.30633395 -1.21027275 -0.6287909 ]\n"
     ]
    }
   ],
   "source": [
    "x_opt_1 = minimize(lambda x: f1(x) + f2(x), x0, method='BFGS', \n",
    "                   jac=lambda x: g1(x) + g2(x), \n",
    "                   options={'maxiter': 100, 'disp': False}, tol=1e-12).x\n",
    "\n",
    "print(x_opt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6074129  -0.30633395 -1.21027275 -0.6287909 ]\n"
     ]
    }
   ],
   "source": [
    "f_list = [f1, f2]\n",
    "g_list = [g1, g2]\n",
    "\n",
    "x_opt_2 = consensus_admm(f_list, g_list, x0)\n",
    "print(x_opt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
